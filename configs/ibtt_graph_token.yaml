dataset:
  graph_token_root: graph-token
  task: cycle_check  # cycle_check, shortest_path
  train_algorithms: [ba, sbm]  # List of algorithms for training/validation
  test_algorithm: sfn  # OOD algorithm for testing
  use_split_tasks_dirs: true
  num_graphs: 500  # Number of graph files to sample per algorithm
  num_pairs_per_graph: 10  # For shortest_path: number of query pairs to sample per graph (null for cycle_check)
  max_len: 600 # maximum sequence length before truncated (cut graph)
  max_vocab: 600 # maximum unique tokens include special/node_id/tasks

model:
  vocab_size: null # set automatically from training data
  d_model: 16
  nhead: 4
  nlayers: 2
  d_ff: 128
  dropout: 0.1
  max_pos: 600 # maximum positional encoding length

train:
  batch_size: 128
  epochs: 100
  lr: 0.001  
  weight_decay: 1.0e-4 
  seed: 0   
  num_workers: 2  

output:
  out_dir: runs_ibtt
  run_name: ibtt-cycle-check

wandb:
  use: true
  project: graph-token
