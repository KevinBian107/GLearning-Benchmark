dataset:
  graph_token_root: graph-token
  task: shortest_path  # cycle_check, shortest_path
  algorithm: er # er, ba, sbm, ws, grid
  use_split_tasks_dirs: true
  data_fraction: 0.5 # fraction of data to use (0.0-1.0), same for train/val/test
  max_len: 600 # maximum sequence length before truncated (cut graph)
  max_vocab: 600 # maximum unique tokens include special/node_id/tasks

  balance_classes: True
  balance_strategy: soft_oversample  # undersample (match minority), median (match median class size), oversample (match majority), soft_oversample (both under/over sample to mean class size)

model:
  vocab_size: null # set automatically from training data
  d_model: 16
  nhead: 4
  nlayers: 2
  d_ff: 128
  dropout: 0.1
  max_pos: 600 # maximum positional encoding length
  num_classes: 7 # 7 for shortest_path (len1-len7), 2 for cycle_check

train:
  batch_size: 128
  epochs: 300
  lr: 0.001  
  weight_decay: 1.0e-4 
  seed: 0   
  num_workers: 2  

output:
  out_dir: runs_ibtt
  run_name: ibtt-shortest-path

wandb:
  use: true
  project: graph-token
