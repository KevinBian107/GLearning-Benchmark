dataset:
  graph_token_root: graph-token
  task: cycle_check  # cycle_check, edge_existence, node_degree, node_count, edge_count, connected_nodes
  algorithm: er      # er, ba, sbm, ws, grid
  use_split_tasks_dirs: true
  max_len: 512       # Maximum sequence length
  max_vocab: 60000   # Maximum vocabulary size

model:
  vocab_size: null   # Will be set automatically from training data
  d_model: 256       # Model dimension
  nhead: 8           # Number of attention heads
  nlayers: 4         # Number of transformer layers
  d_ff: 512          # Feed-forward dimension
  dropout: 0.1       # Dropout rate
  max_pos: 512       # Maximum positional encoding length (should match max_len)

train:
  batch_size: 128
  epochs: 100
  lr: 0.001  
  weight_decay: 1.0e-4 
  seed: 0   
  num_workers: 2  

output:
  out_dir: runs_gtt
  run_name: gtt-cycle-check

wandb:
  use: true
  project: graph-token
