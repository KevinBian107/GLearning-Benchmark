dataset:
  graph_token_root: graph-token
  task: cycle_check  # cycle_check, edge_existence, node_degree, node_count, edge_count, connected_nodes
  algorithm: er # er, ba, sbm, ws, grid
  use_split_tasks_dirs: true
  max_len: 512 # Maximum sequence length before truncated (cut graph)
  max_vocab: 1000 # Maximum unique tokens include special/node_id/tasks

model:
  vocab_size: null # set automatically from training data
  d_model: 64  
  nhead: 8 
  nlayers: 4
  d_ff: 214
  dropout: 0.1
  max_pos: 512 # maximum positional encoding length

train:
  batch_size: 128
  epochs: 100
  lr: 0.001  
  weight_decay: 1.0e-4 
  seed: 0   
  num_workers: 2  

output:
  out_dir: runs_gtt
  run_name: gtt-cycle-check

wandb:
  use: true
  project: graph-token
