{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing GCN & GAT on Node Classification\n",
    "\n",
    "We will be using **Cora**, a citation network dataset, to serve as an example:\n",
    "- Nodes: scientific publications (papers)\n",
    "- Edges: citation links between papers (if paper A cites paper B, there's an edge)\n",
    "- Node Features: 1,433-dimensional bag-of-words feature vectors (word presence/absence in each paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# other stuff\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Loading Cora dataset...\")\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "data = dataset[0]\n",
    "\n",
    "print(f\"Dataset: {dataset}\")\n",
    "print(f\"Number of graphs: {len(dataset)}\")\n",
    "print(f\"Number of nodes: {data.num_nodes}\")\n",
    "print(f\"Number of edges: {data.num_edges}\")\n",
    "print(f\"Number of features: {dataset.num_features}\")\n",
    "print(f\"Number of classes: {dataset.num_classes}\")\n",
    "print(f\"Has isolated nodes: {data.has_isolated_nodes()}\")\n",
    "print(f\"Has self-loops: {data.has_self_loops()}\")\n",
    "print(f\"Is undirected: {data.is_undirected()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Convolution Network (GCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, num_classes)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = GCN(\n",
    "    num_features=dataset.num_features,\n",
    "    hidden_channels=16,\n",
    "    num_classes=dataset.num_classes\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        pred = out.argmax(dim=1)\n",
    "        \n",
    "        train_correct = pred[data.train_mask] == data.y[data.train_mask]\n",
    "        train_acc = int(train_correct.sum()) / int(data.train_mask.sum())\n",
    "        \n",
    "        val_correct = pred[data.val_mask] == data.y[data.val_mask]\n",
    "        val_acc = int(val_correct.sum()) / int(data.val_mask.sum())\n",
    "        \n",
    "        test_correct = pred[data.test_mask] == data.y[data.test_mask]\n",
    "        test_acc = int(test_correct.sum()) / int(data.test_mask.sum())\n",
    "        \n",
    "    return train_acc, val_acc, test_acc, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining GCN model...\")\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in tqdm(range(1, 501), desc=\"Training\", unit=\"epoch\"):\n",
    "    loss = train()\n",
    "    train_acc, val_acc, test_acc, _ = test()\n",
    "    train_losses.append(loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "train_acc, val_acc, test_acc, predictions = test()\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Final Results:\")\n",
    "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(train_losses, color='blue', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Training Loss', fontsize=12)\n",
    "ax1.set_title('Training Loss over Epochs', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(val_accuracies, color='green', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Validation Accuracy', fontsize=12)\n",
    "ax2.set_title('Validation Accuracy over Epochs', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Attention Transformer (GAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, num_classes, heads=8, dropout=0.6, attn_dropout=0.6):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.conv1 = GATConv(num_features, hidden_channels, heads=heads, dropout=attn_dropout)\n",
    "        self.conv2 = GATConv(hidden_channels * heads, num_classes, heads=1, concat=False, dropout=attn_dropout)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = GAT(\n",
    "    num_features=dataset.num_features,\n",
    "    hidden_channels=16,\n",
    "    num_classes=dataset.num_classes,\n",
    "    heads=8,\n",
    "    dropout=0.6,\n",
    "    attn_dropout=0.6\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        pred = out.argmax(dim=1)\n",
    "        train_correct = pred[data.train_mask] == data.y[data.train_mask]\n",
    "        train_acc = int(train_correct.sum()) / int(data.train_mask.sum())\n",
    "        val_correct = pred[data.val_mask] == data.y[data.val_mask]\n",
    "        val_acc = int(val_correct.sum()) / int(data.val_mask.sum())\n",
    "        test_correct = pred[data.test_mask] == data.y[data.test_mask]\n",
    "        test_acc = int(test_correct.sum()) / int(data.test_mask.sum())\n",
    "    \n",
    "    return train_acc, val_acc, test_acc, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining GAT model...\")\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in tqdm(range(1, 501), desc=\"Training\", unit=\"epoch\"):\n",
    "    loss = train()\n",
    "    train_acc, val_acc, test_acc, _ = test()\n",
    "    train_losses.append(loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "train_acc, val_acc, test_acc, predictions = test()\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Final Results:\")\n",
    "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(train_losses, linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Training Loss', fontsize=12)\n",
    "ax1.set_title('Training Loss over Epochs', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(val_accuracies, linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Validation Accuracy', fontsize=12)\n",
    "ax2.set_title('Validation Accuracy over Epochs', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Performance When Layer Increase\n",
    "\n",
    "Model details:\n",
    "\n",
    "- Deep Graph Convolutional Network (Deep GCN):\n",
    "    - Conv + ReLU\n",
    "    - Dropout layer\n",
    "    - Weight decay\n",
    "    - Last layer aggregation\n",
    "\n",
    "- Deep Graph Attention Transformer (Deep GAT)\n",
    "    - Mutihead Attention + LeakyReLU\n",
    "    - Residual + Batchnorm / Layernorm\n",
    "    - Dropout layer\n",
    "    - Weight decay\n",
    "    - Last layer aggregation instead of concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNDeep(nn.Module):\n",
    "    \"\"\"Deep GCN with variable number of layers.\n",
    "    Follow implementation in the GCN (Conv + ReLU) paper but with dropout\"\"\"\n",
    "    \n",
    "    def __init__(self, num_features, hidden_channels, num_classes, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        if num_layers == 1:\n",
    "            self.out = GCNConv(num_features, num_classes)\n",
    "            self.layers = nn.ModuleList()\n",
    "        else:\n",
    "            self.layers = nn.ModuleList()\n",
    "            self.layers.append(GCNConv(num_features, hidden_channels))\n",
    "            for _ in range(num_layers - 2):\n",
    "                self.layers.append(GCNConv(hidden_channels, hidden_channels))\n",
    "            self.out = GCNConv(hidden_channels, num_classes)\n",
    "    \n",
    "    def forward(self, x, edge_index, dropout=0.5):\n",
    "        if self.num_layers == 1:\n",
    "            return self.out(x, edge_index)\n",
    "        for conv in self.layers:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=dropout, training=self.training)\n",
    "        x = self.out(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class GATDeep(nn.Module):\n",
    "    \"\"\"Deep GAT with variable number of layers.\n",
    "    Follow implementation in the GAT paper (Multihead Attention + LeakyReLU + Residual + Batchnorm) but with dropout\"\"\"\n",
    "    \n",
    "    def __init__(self, num_features, hidden_channels, num_classes, num_layers,\n",
    "                 heads=8, dropout=0.6, attn_dropout=0.6, norm_type=\"layer\"):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.norm_type = norm_type.lower()\n",
    "        h_per_head = max(1, hidden_channels // heads)\n",
    "        hidden_dim = h_per_head * heads\n",
    "\n",
    "        if num_layers == 1:\n",
    "            self.out = GATConv(num_features, num_classes, heads=1, concat=False, dropout=attn_dropout)\n",
    "            self.layers, self.norms, self.in_proj = nn.ModuleList(), nn.ModuleList(), None\n",
    "        else:\n",
    "            self.layers = nn.ModuleList()\n",
    "            self.norms = nn.ModuleList()\n",
    "            self.layers.append(GATConv(num_features, h_per_head, heads=heads, dropout=attn_dropout))\n",
    "            for _ in range(num_layers - 2):\n",
    "                self.layers.append(GATConv(hidden_dim, h_per_head, heads=heads, dropout=attn_dropout))\n",
    "            self.out = GATConv(hidden_dim, num_classes, heads=1, concat=False, dropout=attn_dropout)\n",
    "            self.in_proj = nn.Linear(num_features, hidden_dim, bias=False)\n",
    "\n",
    "            for _ in range(num_layers - 1):\n",
    "                if self.norm_type == \"batch\":\n",
    "                    self.norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "                elif self.norm_type == \"layer\":\n",
    "                    self.norms.append(nn.LayerNorm(hidden_dim))\n",
    "                else:\n",
    "                    raise ValueError(\"norm_type must be 'batch' or 'layer'\")\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        if self.num_layers == 1:\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            return self.out(x, edge_index)\n",
    "\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        for i, (conv, norm) in enumerate(zip(self.layers, self.norms)):\n",
    "            h = conv(x, edge_index)\n",
    "            h = norm(h)\n",
    "            h = F.elu(h)\n",
    "            if i == 0:\n",
    "                x = h + self.in_proj(x)\n",
    "            else:\n",
    "                x = h + x\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        x = self.out(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def test(model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        pred = out.argmax(dim=1)\n",
    "        train_acc = (pred[data.train_mask] == data.y[data.train_mask]).float().mean().item()\n",
    "        val_acc = (pred[data.val_mask] == data.y[data.val_mask]).float().mean().item()\n",
    "        test_acc = (pred[data.test_mask] == data.y[data.test_mask]).float().mean().item()\n",
    "    return train_acc, val_acc, test_acc, pred\n",
    "\n",
    "\n",
    "def run_depth_sweep(model_type,\n",
    "                    layer_list,\n",
    "                    epochs=300,\n",
    "                    lr=0.005,\n",
    "                    weight_decay=5e-4,\n",
    "                    hidden=16,\n",
    "                    save_dir=\"saved_models\"):\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    results = []\n",
    "    for L in layer_list:\n",
    "        if model_type == \"GCN\":\n",
    "            model = GCNDeep(dataset.num_features, hidden, dataset.num_classes, num_layers=L)\n",
    "        else:\n",
    "            model = GATDeep(dataset.num_features, hidden, dataset.num_classes, num_layers=L, heads=8, dropout=0.6, attn_dropout=0.6)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        best_val, best_test = -1.0, 0.0\n",
    "        best_pred = None\n",
    "        best_path = os.path.join(save_dir, f\"{model_type}_L{L}.pt\")\n",
    "\n",
    "        for _ in tqdm(range(epochs), desc=f\"{model_type} L={L}\", leave=False):\n",
    "            train(model, optimizer, criterion)\n",
    "            _, val_acc, test_acc, pred = test(model)\n",
    "            if val_acc > best_val:\n",
    "                best_val = val_acc\n",
    "                best_test = test_acc\n",
    "                best_pred = pred.clone()\n",
    "                torch.save(model.state_dict(), best_path)\n",
    "        results.append((L, best_test, best_pred, best_path))\n",
    "    return results\n",
    "\n",
    "layers_to_try = [1, 2, 3, 4]\n",
    "gcn_results = run_depth_sweep(\"GCN\", layers_to_try, epochs=500, lr=0.01, weight_decay=5e-4)\n",
    "gat_results = run_depth_sweep(\"GAT\", layers_to_try, epochs=500, lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions_grid(model_type, results, dataset, data, colors, row_idx, fig, use_hidden=False):\n",
    "    n_cols = len(results)\n",
    "    for j, (L, _, _, path) in enumerate(results):\n",
    "        if model_type == \"GCN\":\n",
    "            model = GCNDeep(dataset.num_features, 16, dataset.num_classes, num_layers=L)\n",
    "        else:\n",
    "            model = GATDeep(dataset.num_features, 16, dataset.num_classes, num_layers=L, heads=8, dropout=0.6, attn_dropout=0.6)\n",
    "        model.load_state_dict(torch.load(path))\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(data.x, data.edge_index)\n",
    "            pred = out.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "        node_embeddings = data.x.cpu().numpy()\n",
    "        pca = PCA(n_components=2, random_state=42)\n",
    "        pos = pca.fit_transform(node_embeddings)\n",
    "\n",
    "        ax = fig.axes[row_idx * n_cols + j]\n",
    "        for class_id in range(dataset.num_classes):\n",
    "            nodes = np.where(pred == class_id)[0]\n",
    "            coords = pos[nodes]\n",
    "            ax.scatter(coords[:, 0], coords[:, 1],\n",
    "                       c=colors[class_id], s=25, alpha=0.7, edgecolors='black', linewidth=0.3)\n",
    "        ax.set_title(f\"{model_type} (L={L})\", fontsize=12, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "\n",
    "fig, axes = plt.subplots(2, len(layers_to_try), figsize=(5 * len(layers_to_try), 8))\n",
    "colors = ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3', '#ff7f00', '#ffff33', '#a65628'][:dataset.num_classes]\n",
    "\n",
    "plot_predictions_grid(\"GCN\", gcn_results, dataset, data, colors, row_idx=0, fig=fig)\n",
    "plot_predictions_grid(\"GAT\", gat_results, dataset, data, colors, row_idx=1, fig=fig)\n",
    "\n",
    "fig.suptitle(\"Predicted Node Clusters (PCA Projection)\\nTop: GCN | Bottom: GAT\", fontsize=16, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gcn = [L for L, _, _, _ in gcn_results]\n",
    "y_gcn = [acc for _, acc, _, _ in gcn_results]\n",
    "\n",
    "x_gat = [L for L, _, _, _ in gat_results]\n",
    "y_gat = [acc for _, acc, _, _ in gat_results]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x_gcn, y_gcn, marker='o', linewidth=2, label='GCN', color='#377eb8')\n",
    "plt.plot(x_gat, y_gat, marker='o', linewidth=2, label='GAT', color='#e41a1c')\n",
    "plt.title(\"Test Accuracy vs. Number of Layers\", fontsize=15, fontweight='bold')\n",
    "plt.xlabel(\"Number of Layers\", fontsize=12)\n",
    "plt.ylabel(\"Test Accuracy\", fontsize=12)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_curve(model_type, layer_list, dataset, data, hidden=16, save_dir=\"saved_models\"):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    losses = {}\n",
    "    for L in layer_list:\n",
    "        if model_type == \"GCN\":\n",
    "            model = GCNDeep(dataset.num_features, hidden, dataset.num_classes, num_layers=L)\n",
    "        else:\n",
    "            model = GATDeep(dataset.num_features, hidden, dataset.num_classes,\n",
    "                            num_layers=L, heads=8, dropout=0.6, attn_dropout=0.6)\n",
    "\n",
    "        path = os.path.join(save_dir, f\"{model_type}_L{L}.pt\")\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"{path} not found, skipping.\")\n",
    "            continue\n",
    "\n",
    "        model.load_state_dict(torch.load(path))\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(data.x, data.edge_index)\n",
    "            loss = criterion(out[data.train_mask], data.y[data.train_mask]).item()\n",
    "            val_loss = criterion(out[data.val_mask], data.y[data.val_mask]).item()\n",
    "            test_loss = criterion(out[data.test_mask], data.y[data.test_mask]).item()\n",
    "\n",
    "        losses[L] = {\"train\": loss, \"val\": val_loss, \"test\": test_loss}\n",
    "    return losses\n",
    "\n",
    "layers_to_try = [1, 2, 3, 4]\n",
    "gcn_losses = compute_loss_curve(\"GCN\", layers_to_try, dataset, data)\n",
    "gat_losses = compute_loss_curve(\"GAT\", layers_to_try, dataset, data)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "x = list(gcn_losses.keys())\n",
    "train = [gcn_losses[L][\"train\"] for L in x]\n",
    "val = [gcn_losses[L][\"val\"] for L in x]\n",
    "test = [gcn_losses[L][\"test\"] for L in x]\n",
    "axes[0].plot(x, train, '-o', label=\"Train Loss\", linewidth=2)\n",
    "axes[0].plot(x, val, '-o', label=\"Val Loss\", linewidth=2)\n",
    "axes[0].plot(x, test, '-o', label=\"Test Loss\", linewidth=2)\n",
    "axes[0].set_title(\"GCN Loss vs. Layers\", fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel(\"Number of Layers\"); axes[0].set_ylabel(\"Cross Entropy Loss\")\n",
    "axes[0].grid(True, alpha=0.3); axes[0].legend()\n",
    "\n",
    "x = list(gat_losses.keys())\n",
    "train = [gat_losses[L][\"train\"] for L in x]\n",
    "val = [gat_losses[L][\"val\"] for L in x]\n",
    "test = [gat_losses[L][\"test\"] for L in x]\n",
    "axes[1].plot(x, train, '-o', label=\"Train Loss\", linewidth=2)\n",
    "axes[1].plot(x, val, '-o', label=\"Val Loss\", linewidth=2)\n",
    "axes[1].plot(x, test, '-o', label=\"Test Loss\", linewidth=2)\n",
    "axes[1].set_title(\"GAT Loss vs. Layers\", fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel(\"Number of Layers\"); axes[1].set_ylabel(\"Cross Entropy Loss\")\n",
    "axes[1].grid(True, alpha=0.3); axes[1].legend()\n",
    "\n",
    "plt.suptitle(\"Loss Comparison by Depth â€” GCN vs GAT\", fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glearning_180a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
